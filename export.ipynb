{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "16fe6eb0",
   "metadata": {},
   "source": [
    "This merges the splits and subsets into a single dataset dict and pushes it to HuggingFace Hub."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "e1036e76",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/andrew/Workspace/AmiGO/.venv/lib/python3.12/site-packages/tqdm/auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n",
      "Generating train split: 91220 examples [00:00, 462200.40 examples/s]\n",
      "Casting to class labels: 100%|██████████| 91220/91220 [00:00<00:00, 425012.59 examples/s]\n",
      "Saving the dataset (1/1 shards): 100%|██████████| 82098/82098 [00:00<00:00, 192092.88 examples/s]\n",
      "Saving the dataset (1/1 shards): 100%|██████████| 9122/9122 [00:00<00:00, 197111.07 examples/s]\n",
      "Creating parquet from Arrow format: 100%|██████████| 83/83 [00:01<00:00, 54.21ba/s]\n",
      "Uploading the dataset shards: 100%|██████████| 1/1 [00:13<00:00, 13.26s/it]\n",
      "Creating parquet from Arrow format: 100%|██████████| 10/10 [00:00<00:00, 58.15ba/s]\n",
      "Uploading the dataset shards: 100%|██████████| 1/1 [00:04<00:00,  4.97s/it]\n",
      "Generating train split: 51403 examples [00:00, 819596.62 examples/s]\n",
      "Casting to class labels: 100%|██████████| 51403/51403 [00:00<00:00, 518560.46 examples/s]\n",
      "Saving the dataset (1/1 shards): 100%|██████████| 46262/46262 [00:00<00:00, 225691.48 examples/s]\n",
      "Saving the dataset (1/1 shards): 100%|██████████| 5141/5141 [00:00<00:00, 209103.06 examples/s]\n",
      "Creating parquet from Arrow format: 100%|██████████| 47/47 [00:00<00:00, 59.77ba/s]\n",
      "Uploading the dataset shards: 100%|██████████| 1/1 [00:06<00:00,  6.58s/it]\n",
      "Creating parquet from Arrow format: 100%|██████████| 6/6 [00:00<00:00, 61.34ba/s]\n",
      "Uploading the dataset shards: 100%|██████████| 1/1 [00:01<00:00,  1.12s/it]\n",
      "Generating train split: 68638 examples [00:00, 564299.43 examples/s]\n",
      "Casting to class labels: 100%|██████████| 68638/68638 [00:00<00:00, 431845.70 examples/s]\n",
      "Saving the dataset (1/1 shards): 100%|██████████| 61774/61774 [00:00<00:00, 201055.75 examples/s]\n",
      "Saving the dataset (1/1 shards): 100%|██████████| 6864/6864 [00:00<00:00, 194438.31 examples/s]\n",
      "Creating parquet from Arrow format: 100%|██████████| 62/62 [00:01<00:00, 54.66ba/s]\n",
      "Uploading the dataset shards: 100%|██████████| 1/1 [00:09<00:00,  9.99s/it]\n",
      "Creating parquet from Arrow format: 100%|██████████| 7/7 [00:00<00:00, 53.26ba/s]\n",
      "Uploading the dataset shards: 100%|██████████| 1/1 [00:01<00:00,  1.59s/it]\n",
      "Generating train split: 66702 examples [00:00, 906279.45 examples/s]\n",
      "Casting to class labels: 100%|██████████| 66702/66702 [00:00<00:00, 499707.90 examples/s]\n",
      "Saving the dataset (1/1 shards): 100%|██████████| 60031/60031 [00:00<00:00, 196388.30 examples/s]\n",
      "Saving the dataset (1/1 shards): 100%|██████████| 6671/6671 [00:00<00:00, 211897.38 examples/s]\n",
      "Creating parquet from Arrow format: 100%|██████████| 61/61 [00:00<00:00, 61.09ba/s]\n",
      "Uploading the dataset shards: 100%|██████████| 1/1 [00:08<00:00,  8.51s/it]\n",
      "Creating parquet from Arrow format: 100%|██████████| 7/7 [00:00<00:00, 58.71ba/s]\n",
      "Uploading the dataset shards: 100%|██████████| 1/1 [00:01<00:00,  1.37s/it]\n"
     ]
    }
   ],
   "source": [
    "from datasets import load_dataset\n",
    "\n",
    "dataset_name = \"andrewdalpino/AmiGO\"\n",
    "\n",
    "all_dataset_path = \"./dataset/all-stratified.jsonl\"\n",
    "mf_dataset_path = \"./dataset/mf-stratified.jsonl\"\n",
    "bp_dataset_path = \"./dataset/bp-stratified.jsonl\"\n",
    "cc_dataset_path = \"./dataset/cc-stratified.jsonl\"\n",
    "\n",
    "test_ratio = 0.1\n",
    "\n",
    "random_seed = 42\n",
    "\n",
    "for subset_name, dataset_path in [\n",
    "    (\"all\", all_dataset_path),\n",
    "    (\"mf\", mf_dataset_path),\n",
    "    (\"bp\", bp_dataset_path),\n",
    "    (\"cc\", cc_dataset_path),\n",
    "]:\n",
    "    dataset = load_dataset(\"json\", data_files=dataset_path)\n",
    "\n",
    "    dataset = dataset.class_encode_column(\"stratum_id\")\n",
    "\n",
    "    dataset = dataset[\"train\"].train_test_split(test_size=test_ratio, stratify_by_column=\"stratum_id\", seed=random_seed)\n",
    "\n",
    "    dataset.save_to_disk(f\"./exports/{dataset_name}/{subset_name}\")\n",
    "\n",
    "    dataset.push_to_hub(dataset_name, subset_name)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
