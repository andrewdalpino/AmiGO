{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "16fe6eb0",
   "metadata": {},
   "source": [
    "This merges the splits and subsets into a single dataset dict and pushes it to HuggingFace Hub."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "e1036e76",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/andrew/Workspace/AminoGOGO/.venv/lib/python3.12/site-packages/tqdm/auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n",
      "Generating train split: 91195 examples [00:00, 812426.97 examples/s]\n",
      "Casting to class labels: 100%|██████████| 91195/91195 [00:00<00:00, 530133.15 examples/s]\n",
      "Saving the dataset (1/1 shards): 100%|██████████| 82075/82075 [00:00<00:00, 241281.05 examples/s]\n",
      "Saving the dataset (1/1 shards): 100%|██████████| 9120/9120 [00:00<00:00, 238914.06 examples/s]\n",
      "Creating parquet from Arrow format: 100%|██████████| 83/83 [00:01<00:00, 60.09ba/s]\n",
      "Uploading the dataset shards: 100%|██████████| 1/1 [00:11<00:00, 11.60s/it]\n",
      "Creating parquet from Arrow format: 100%|██████████| 10/10 [00:00<00:00, 64.15ba/s]\n",
      "Uploading the dataset shards: 100%|██████████| 1/1 [00:01<00:00,  1.67s/it]\n",
      "Generating train split: 51376 examples [00:00, 905280.20 examples/s]\n",
      "Casting to class labels: 100%|██████████| 51376/51376 [00:00<00:00, 569672.40 examples/s]\n",
      "Saving the dataset (1/1 shards): 100%|██████████| 46238/46238 [00:00<00:00, 244646.88 examples/s]\n",
      "Saving the dataset (1/1 shards): 100%|██████████| 5138/5138 [00:00<00:00, 236121.47 examples/s]\n",
      "Creating parquet from Arrow format: 100%|██████████| 47/47 [00:00<00:00, 65.20ba/s]\n",
      "Uploading the dataset shards: 100%|██████████| 1/1 [00:06<00:00,  6.49s/it]\n",
      "Creating parquet from Arrow format: 100%|██████████| 6/6 [00:00<00:00, 67.23ba/s]\n",
      "Uploading the dataset shards: 100%|██████████| 1/1 [00:01<00:00,  1.81s/it]\n",
      "Generating train split: 68517 examples [00:00, 992132.59 examples/s]\n",
      "Casting to class labels: 100%|██████████| 68517/68517 [00:00<00:00, 553551.28 examples/s]\n",
      "Saving the dataset (1/1 shards): 100%|██████████| 61665/61665 [00:00<00:00, 243739.29 examples/s]\n",
      "Saving the dataset (1/1 shards): 100%|██████████| 6852/6852 [00:00<00:00, 245302.29 examples/s]\n",
      "Creating parquet from Arrow format: 100%|██████████| 62/62 [00:00<00:00, 63.40ba/s]\n",
      "Uploading the dataset shards: 100%|██████████| 1/1 [00:09<00:00,  9.02s/it]\n",
      "Creating parquet from Arrow format: 100%|██████████| 7/7 [00:00<00:00, 57.61ba/s]\n",
      "Uploading the dataset shards: 100%|██████████| 1/1 [00:01<00:00,  1.32s/it]\n",
      "Generating train split: 66690 examples [00:00, 1093734.15 examples/s]\n",
      "Casting to class labels: 100%|██████████| 66690/66690 [00:00<00:00, 556105.20 examples/s]\n",
      "Saving the dataset (1/1 shards): 100%|██████████| 60021/60021 [00:00<00:00, 244540.68 examples/s]\n",
      "Saving the dataset (1/1 shards): 100%|██████████| 6669/6669 [00:00<00:00, 240247.82 examples/s]\n",
      "Creating parquet from Arrow format: 100%|██████████| 61/61 [00:00<00:00, 61.60ba/s]\n",
      "Uploading the dataset shards: 100%|██████████| 1/1 [00:08<00:00,  8.34s/it]\n",
      "Creating parquet from Arrow format: 100%|██████████| 7/7 [00:00<00:00, 63.77ba/s]\n",
      "Uploading the dataset shards: 100%|██████████| 1/1 [00:01<00:00,  1.30s/it]\n"
     ]
    }
   ],
   "source": [
    "from datasets import load_dataset\n",
    "\n",
    "dataset_name = \"andrewdalpino/AmiGO\"\n",
    "\n",
    "all_dataset_path = \"./dataset/all-stratified.jsonl\"\n",
    "mf_dataset_path = \"./dataset/mf-stratified.jsonl\"\n",
    "bp_dataset_path = \"./dataset/bp-stratified.jsonl\"\n",
    "cc_dataset_path = \"./dataset/cc-stratified.jsonl\"\n",
    "\n",
    "test_ratio = 0.1\n",
    "\n",
    "random_seed = 42\n",
    "\n",
    "for subset_name, dataset_path in [\n",
    "    (\"all\", all_dataset_path),\n",
    "    (\"mf\", mf_dataset_path),\n",
    "    (\"bp\", bp_dataset_path),\n",
    "    (\"cc\", cc_dataset_path),\n",
    "]:\n",
    "    dataset = load_dataset(\"json\", data_files=dataset_path)\n",
    "\n",
    "    dataset = dataset.class_encode_column(\"stratum_id\")\n",
    "\n",
    "    dataset = dataset[\"train\"].train_test_split(test_size=test_ratio, stratify_by_column=\"stratum_id\", seed=random_seed)\n",
    "\n",
    "    dataset.save_to_disk(f\"./exports/{dataset_name}/{subset_name}\")\n",
    "\n",
    "    dataset.push_to_hub(dataset_name, subset_name)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
