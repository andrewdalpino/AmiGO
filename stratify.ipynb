{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "07c0b691",
   "metadata": {},
   "source": [
    "Let's cluster the samples in the SwissProt GO dataset by their GO terms so we can use the cluster assignment to later do a stratified train/test split. We'll start by creating some embeddings for the terms."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "79c7eb6e",
   "metadata": {},
   "outputs": [],
   "source": [
    "from functools import partial\n",
    "\n",
    "from datasets import load_dataset\n",
    "\n",
    "import numpy as np\n",
    "\n",
    "from sklearn.decomposition import TruncatedSVD\n",
    "from sklearn.mixture import GaussianMixture\n",
    "\n",
    "from collections import Counter\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "all_dataset_path = \"./dataset/all-expanded.jsonl\"\n",
    "mf_dataset_path = \"./dataset/mf-expanded.jsonl\"\n",
    "bp_dataset_path = \"./dataset/bp-expanded.jsonl\"\n",
    "cc_dataset_path = \"./dataset/cc-expanded.jsonl\"\n",
    "\n",
    "all_dimensions = 768\n",
    "mf_dimensions = 256\n",
    "bp_dimensions = 512\n",
    "cc_dimensions = 128\n",
    "\n",
    "num_strata = 100\n",
    "\n",
    "random_seed = 42\n",
    "\n",
    "new_svd = partial(TruncatedSVD, random_state=random_seed)\n",
    "\n",
    "new_gmm = partial(\n",
    "    GaussianMixture,\n",
    "    n_components=num_strata,\n",
    "    covariance_type=\"diag\",\n",
    "    random_state=random_seed,\n",
    "    max_iter=300,\n",
    "    verbose=2,\n",
    ")\n",
    "\n",
    "all_stratum_ids = {}\n",
    "mf_stratum_ids = {}\n",
    "bp_stratum_ids = {}\n",
    "cc_stratum_ids = {}\n",
    "\n",
    "all_counter = Counter()\n",
    "mf_counter = Counter()\n",
    "bp_counter = Counter()\n",
    "cc_counter = Counter()\n",
    "\n",
    "for name, dataset_path, stratum_ids, dimensions, counter in [\n",
    "    (\"All\", all_dataset_path, all_stratum_ids, all_dimensions, all_counter),\n",
    "    (\"Molecular Function\", mf_dataset_path, mf_stratum_ids, mf_dimensions, mf_counter),\n",
    "    (\"Biological Process\", bp_dataset_path, bp_stratum_ids, bp_dimensions, bp_counter),\n",
    "    (\"Cellular Component\", cc_dataset_path, cc_stratum_ids, cc_dimensions, cc_counter),\n",
    "]:\n",
    "    dataset = load_dataset(\"json\", data_files=dataset_path, split=\"train\")\n",
    "\n",
    "    unique_terms = set()\n",
    "\n",
    "    for record in dataset:\n",
    "        for term in record[\"go_terms\"]:\n",
    "            unique_terms.add(term)\n",
    "\n",
    "    term_index_mapping = {term: index for index, term in enumerate(unique_terms)}\n",
    "\n",
    "    term_embeddings = {}\n",
    "\n",
    "    for record in dataset:\n",
    "        id = record[\"id\"]\n",
    "\n",
    "        embedding = np.zeros(len(term_index_mapping), dtype=np.int8)\n",
    "\n",
    "        for term in record[\"go_terms\"]:\n",
    "            if term in term_index_mapping:\n",
    "                index = term_index_mapping[term]\n",
    "\n",
    "                embedding[index] = 1\n",
    "\n",
    "        term_embeddings[id] = embedding\n",
    "\n",
    "    svd = new_svd(n_components=dimensions)\n",
    "\n",
    "    x = np.stack(list(term_embeddings.values()))\n",
    "    \n",
    "    svd.fit(x)\n",
    "    \n",
    "    explained_variance = np.sum(svd.explained_variance_ratio_)\n",
    "\n",
    "    print(f\"Embedding dimensionality: {dimensions}, Explained Variance: {explained_variance:.4f}\")\n",
    "\n",
    "    z = svd.transform(x)\n",
    "\n",
    "    for sequence_id, embedding in zip(term_embeddings.keys(), z):\n",
    "        term_embeddings[sequence_id] = embedding\n",
    "\n",
    "    print(\"\\n\")\n",
    "\n",
    "    gmm = new_gmm()\n",
    "\n",
    "    x = np.stack(list(term_embeddings.values()))\n",
    "\n",
    "    predictions = gmm.fit_predict(x)\n",
    "\n",
    "    for sequence_id, stratum_id in zip(term_embeddings.keys(), predictions):\n",
    "        stratum_ids[sequence_id] = stratum_id\n",
    "\n",
    "        counter[stratum_id] += 1\n",
    "\n",
    "    plt.figure(figsize=(12, 5)) \n",
    "\n",
    "    plt.bar(counter.keys(), counter.values())\n",
    "\n",
    "    plt.title(f\"{name} Stratum Frequencies\")\n",
    "    plt.xticks(rotation=90)\n",
    "    plt.xlabel(\"Stratum ID\")\n",
    "    plt.ylabel(\"Frequency\")\n",
    "\n",
    "    plt.show()\n",
    "\n",
    "    print(\"\\n\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0b86e311",
   "metadata": {},
   "source": [
    "Lastly, add the stratum IDs to the dataset, dropping any singletons, and write to disk."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2418f499",
   "metadata": {},
   "outputs": [],
   "source": [
    "import json\n",
    "\n",
    "all_stratified_path = \"./dataset/all-stratified.jsonl\"\n",
    "mf_stratified_path = \"./dataset/mf-stratified.jsonl\"\n",
    "bp_stratified_path = \"./dataset/bp-stratified.jsonl\"\n",
    "cc_stratified_path = \"./dataset/cc-stratified.jsonl\"\n",
    "\n",
    "for dataset_path, stratified_path, stratum_ids, counter in [\n",
    "    (all_dataset_path, all_stratified_path, all_stratum_ids, all_counter),\n",
    "    (mf_dataset_path, mf_stratified_path, mf_stratum_ids, mf_counter),\n",
    "    (bp_dataset_path, bp_stratified_path, bp_stratum_ids, bp_counter),\n",
    "    (cc_dataset_path, cc_stratified_path, cc_stratum_ids, cc_counter),\n",
    "]:\n",
    "    dataset = load_dataset(\"json\", data_files=dataset_path, split=\"train\")\n",
    "\n",
    "    dropped = 0\n",
    "\n",
    "    with open(stratified_path, \"w\") as file:\n",
    "        for record in dataset:\n",
    "            id = record[\"id\"]\n",
    "            stratum_id = stratum_ids[id]\n",
    "\n",
    "            if counter[stratum_id] < 2:\n",
    "                dropped += 1\n",
    "\n",
    "                continue\n",
    "\n",
    "            record[\"stratum_id\"] = str(stratum_id)\n",
    "\n",
    "            file.write(json.dumps(record) + \"\\n\")\n",
    "\n",
    "    print(f\"Dropped {dropped:,} noise samples from dataset.\")\n",
    "\n",
    "    print(f\"Dataset saved to {stratified_path}\")\n",
    "\n",
    "print(\"Done!\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
