{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "07c0b691",
   "metadata": {},
   "source": [
    "Let's cluster the samples in the SwissProt GO dataset by their GO terms so we can use the cluster assignment to later do a stratified train/test split. We'll start by creating some embeddings for the terms."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "79c7eb6e",
   "metadata": {},
   "outputs": [],
   "source": [
    "from functools import partial\n",
    "\n",
    "from datasets import load_dataset\n",
    "\n",
    "import numpy as np\n",
    "\n",
    "from sklearn.decomposition import TruncatedSVD\n",
    "\n",
    "all_dataset_path = \"./dataset/all-expanded.jsonl\"\n",
    "mf_dataset_path = \"./dataset/mf-expanded.jsonl\"\n",
    "bp_dataset_path = \"./dataset/bp-expanded.jsonl\"\n",
    "cc_dataset_path = \"./dataset/cc-expanded.jsonl\"\n",
    "\n",
    "random_seed = 42\n",
    "\n",
    "min_explained_variance = 0.5\n",
    "\n",
    "all_terms_embeddings = {}\n",
    "mf_terms_embeddings = {}\n",
    "bp_terms_embeddings = {}\n",
    "cc_terms_embeddings = {}\n",
    "\n",
    "new_svd = partial(TruncatedSVD, n_iter=7, random_state=random_seed)\n",
    "\n",
    "for dataset_path, terms_embeddings in [\n",
    "    (all_dataset_path, all_terms_embeddings),\n",
    "    (mf_dataset_path, mf_terms_embeddings),\n",
    "    (bp_dataset_path, bp_terms_embeddings),\n",
    "    (cc_dataset_path, cc_terms_embeddings),\n",
    "]:\n",
    "    dataset = load_dataset(\"json\", data_files=dataset_path, split=\"train\")\n",
    "\n",
    "    unique_terms = set()\n",
    "\n",
    "    for record in dataset:\n",
    "        for term in record[\"terms\"]:\n",
    "            unique_terms.add(term)\n",
    "\n",
    "    term_index_mapping = {term: index for index, term in enumerate(unique_terms)}\n",
    "\n",
    "    for record in dataset:\n",
    "        id = record[\"id\"]\n",
    "\n",
    "        embedding = np.zeros(len(term_index_mapping), dtype=np.int8)\n",
    "\n",
    "        for term in record[\"terms\"]:\n",
    "            if term in term_index_mapping:\n",
    "                index = term_index_mapping[term]\n",
    "\n",
    "                embedding[index] = 1\n",
    "\n",
    "        terms_embeddings[id] = embedding\n",
    "\n",
    "    best_dimensionality = 0\n",
    "    best_explained_variance = 0\n",
    "    best_model = None\n",
    "\n",
    "    for dimensionality in (32, 48, 64):\n",
    "        svd = new_svd(n_components=dimensionality)\n",
    "\n",
    "        x = np.stack(list(terms_embeddings.values()))\n",
    "        \n",
    "        svd.fit(x)\n",
    "        \n",
    "        explained_variance = np.sum(svd.explained_variance_ratio_)\n",
    "\n",
    "        if explained_variance > best_explained_variance:\n",
    "            best_dimensionality = dimensionality\n",
    "            best_explained_variance = explained_variance\n",
    "            best_model = svd\n",
    "\n",
    "        if explained_variance >= min_explained_variance:\n",
    "            break\n",
    "\n",
    "    z = best_model.transform(x)\n",
    "\n",
    "    for sequence_id, embedding in zip(terms_embeddings.keys(), z):\n",
    "        terms_embeddings[sequence_id] = embedding\n",
    "\n",
    "    print(f\"Best dimensionality: {best_dimensionality}\")\n",
    "    print(f\"Explained variance ratio: {best_explained_variance:.2f}\")\n",
    "\n",
    "    print(\"\\n\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7df4f246",
   "metadata": {},
   "source": [
    "With the fresh embeddings, we'll cluster the sequences into strata of similar GO subgraphs."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "690dbfdb",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.cluster import KMeans\n",
    "\n",
    "from collections import Counter\n",
    "\n",
    "from functools import partial\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "num_strata = 100\n",
    "\n",
    "all_stratum_ids = {}\n",
    "mf_stratum_ids = {}\n",
    "bp_stratum_ids = {}\n",
    "cc_stratum_ids = {}\n",
    "\n",
    "new_kmeans = partial(KMeans, random_state=random_seed)\n",
    "\n",
    "for name, terms_embeddings, stratum_ids in [\n",
    "    (\"All\", all_terms_embeddings, all_stratum_ids),\n",
    "    (\"Molecular Function\", mf_terms_embeddings, mf_stratum_ids),\n",
    "    (\"Biological Process\", bp_terms_embeddings, bp_stratum_ids),\n",
    "    (\"Cellular Component\", cc_terms_embeddings, cc_stratum_ids),\n",
    "]:\n",
    "    kmeans = new_kmeans(n_clusters=num_strata)\n",
    "\n",
    "    x = np.stack(list(terms_embeddings.values()))\n",
    "\n",
    "    kmeans.fit(x)\n",
    "\n",
    "    strata_ids = kmeans.predict(x)\n",
    "\n",
    "    counter = Counter()\n",
    "\n",
    "    for sequence_id, stratum_id in zip(terms_embeddings.keys(), strata_ids):\n",
    "        stratum_ids[sequence_id] = stratum_id\n",
    "\n",
    "        counter[stratum_id] += 1\n",
    "\n",
    "    plt.figure(figsize=(12, 5)) \n",
    "\n",
    "    plt.bar(counter.keys(), counter.values())\n",
    "\n",
    "    plt.title(f\"{name} Stratum Frequencies\")\n",
    "    plt.xticks(rotation=90)\n",
    "    plt.xlabel(\"Stratum ID\")\n",
    "    plt.ylabel(\"Frequency\")\n",
    "\n",
    "    plt.show()\n",
    "\n",
    "    print(f\"K-means steps: {kmeans.n_iter_}\")\n",
    "    print(f\"Inertia Loss: {kmeans.inertia_}\")\n",
    "\n",
    "    print(\"\\n\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0b86e311",
   "metadata": {},
   "source": [
    "Lastly, add the term embeddings and stratum IDs to the dataset and write to disk."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2418f499",
   "metadata": {},
   "outputs": [],
   "source": [
    "import json\n",
    "\n",
    "all_stratified_path = \"./dataset/all-stratified.jsonl\"\n",
    "mf_stratified_path = \"./dataset/mf-stratified.jsonl\"\n",
    "bp_stratified_path = \"./dataset/bp-stratified.jsonl\"\n",
    "cc_stratified_path = \"./dataset/cc-stratified.jsonl\"\n",
    "\n",
    "for dataset_path, stratified_path, terms_embeddings, stratum_ids in [\n",
    "    (all_dataset_path, all_stratified_path, all_terms_embeddings, all_stratum_ids),\n",
    "    (mf_dataset_path, mf_stratified_path, mf_terms_embeddings, mf_stratum_ids),\n",
    "    (bp_dataset_path, bp_stratified_path, bp_terms_embeddings, bp_stratum_ids),\n",
    "    (cc_dataset_path, cc_stratified_path, cc_terms_embeddings, cc_stratum_ids),\n",
    "]:\n",
    "    dataset = load_dataset(\"json\", data_files=dataset_path, split=\"train\")\n",
    "\n",
    "    for record in dataset:\n",
    "        id = record[\"id\"]\n",
    "\n",
    "        record[\"terms_embedding\"] = terms_embeddings[id].tolist()\n",
    "        record[\"stratum_id\"] = stratum_ids[id]\n",
    "\n",
    "    with open(stratified_path, \"w\") as file:\n",
    "        for record in dataset:\n",
    "            file.write(json.dumps(record) + \"\\n\")\n",
    "\n",
    "    print(f\"Dataset saved to {stratified_path}\")\n",
    "\n",
    "print(\"Done!\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "name": "python",
   "version": "3.12.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
